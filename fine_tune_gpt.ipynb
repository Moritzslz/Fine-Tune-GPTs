{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt6QP+gzVEnUgq/0DHcjh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moritzslz/Fine-Tune-GPTs/blob/main/fine_tune_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised OpenAI GPT fine-tuning by Moritz Schultz\n",
        "\n",
        "*   www.moritzschultz.de\n",
        "\n",
        "*   https://github.com/Moritzslz\n",
        "\n",
        "\n",
        "I created this notebook in order to easily create training datasets using GPT-4 in order to fine-tune GPT-3.5-turbo.\n",
        "\n",
        "\n",
        "# Here is how it works:\n",
        "\n",
        "# Step 1: Set Your API Key\n",
        "\n",
        "First, you need to set your OpenAI API key. This key allows the notebook to interact with the OpenAI API.\n",
        "\n",
        "# Step 2: Define the Number of Datasets\n",
        "\n",
        "Specify the number of datasets (prompt-response pairs) you want to generate. In order to fine-tune GPT-3.5-turbo it is recommended to have at least 50 training datasets.\n",
        "\n",
        "# Step 3: Set the Temperature\n",
        "\n",
        "The temperature controls the randomness of the model's output. A higher value (e.g., 1) makes the output more random, while a lower value (e.g., 0.2) makes it more deterministic.\n",
        "\n",
        "# Step 4: Set the Maximum Tokens per Request\n",
        "\n",
        "Define the maximum number of tokens the model can use in a single response. This helps in controlling the length of the generated content.\n",
        "\n",
        "# Step 5: Define the Fine-Tune Task Description\n",
        "\n",
        "Provide a detailed description of the task for which you want to fine-tune the model. This description will guide the generation of relevant prompt-response pairs.\n",
        "\n",
        "\n",
        "\n",
        "# Learn about OpenAIs pricing and when it makes sense to fine-tune\n",
        "\n",
        "Before you create a fine-tuned model it maked sense to calculate if it makes sense from cost perspective. Generating a dataset with 50 training examples costs about 8€ using GPT-4.\n",
        "\n",
        "You can find OpenAIs pricing information [here](https://openai.com/api/pricing/).\n",
        "\n",
        "Furthermore I recommend [this](https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning) documentation from OpenAI about when to use fine-tuning."
      ],
      "metadata": {
        "id": "6kaK-OxHOpdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "your_api_key = \"\"\n",
        "# Step 2\n",
        "number_of_datasets_to_generate = 3\n",
        "# Step 3\n",
        "temperature = 1\n",
        "# Step 4\n",
        "maximum_tokens_per_request = 1024\n",
        "# Step 5 (this is an example)\n",
        "fine_tune_task_description = \"I aim to fine-tune a model to classify customer support tickets into predefined categories. The objective is to automate the initial triaging process, directing each ticket to the appropriate department or support team, thereby improving efficiency and response times in addressing customer inquiries. The predefined categories are: Technical Support, Billing and Payment Inquiries, Product Information and Features, Order Status and Shipping, Account Management, General Inquiries and FAQs, Feedback and Suggestions, Complaints and Escalations.\""
      ],
      "metadata": {
        "id": "4a1CVzXwasnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "V6WwaA_GJg-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2dc77b2d-bf0b-4a0d-8c24-cda744eca29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as re\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=your_api_key)\n",
        "\n",
        "def generate_system_message(client, fine_tune_task_description):\n",
        "  messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": 'You are a very beneficial assistant for creating training data used for OpenAI fine-tuning. Please generate a system message based on the detailed description of the model I want to train according to this json schema: {\"role\": \"system\", \"content\": \"$system_message_goes_here\"}. Please return the json in one line. Remember that you are not generating the system message for data generation but a fitting concise and detailed system message for the fine-tuned modle to use as instructions.'},\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Please take your time and work on this step by step. Please generate the system message based on this fine-tune model description: \" + fine_tune_task_description}\n",
        "  ]\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      response_format={ \"type\": \"json_object\" },\n",
        "      messages=messages\n",
        "    )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "system_message = generate_system_message(client, fine_tune_task_description);\n",
        "\n",
        "print(system_message)"
      ],
      "metadata": {
        "id": "Gh0MVhp9Pdh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14de11e-21fd-433a-d259-f4e55cbc8e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"role\": \"system\", \"content\": \"You are tasked with classifying customer support tickets into one of the predefined categories to automate the initial triaging process. The categories to classify tickets into are: Technical Support, Billing and Payment Inquiries, Product Information and Features, Order Status and Shipping, Account Management, General Inquiries and FAQs, Feedback and Suggestions, Complaints and Escalations. Your accurate classification will help direct each ticket to the appropriate department or support team, thereby improving efficiency and response times in addressing customer inquiries.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def generate_training_dataset(client, number_of_datasets_to_generate):\n",
        "\n",
        "  messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": 'You are a very beneficial assistant for creating training data used for OpenAI fine-tuning. Please generate training data that can be used to train machine learning models. You will be given a detailed description of the model I want to train. Based on that description please generate prompt-response pairs according to this json schema: {\"messages\": [{\"role\": \"user\", \"content\": \"$prompt_goes_here\"}, {\"role\": \"assistant\", \"content\": \"$response_goes_here\"}]}. Please return the json in one line. Only one prompt-response pair should be generated per request. Make sure that the sample data is diverse but high quality. With each following request increase the complexity of the generated prompt-response pair, please. Please try to have a great diversity. Description of the model: ' + fine_tune_task_description},\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Please take your time and work on this step by step. Please generate the next training dataset and make sure it's diverse compared to the ones before.\"}\n",
        "  ]\n",
        "\n",
        "  training_dataset = []\n",
        "\n",
        "  for i in range(number_of_datasets_to_generate):\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      response_format={ \"type\": \"json_object\" },\n",
        "      messages=messages\n",
        "    )\n",
        "    assistant_response = completion.choices[0].message.content\n",
        "    response_data = json.loads(assistant_response)\n",
        "    response_data[\"messages\"].insert(0, system_message)\n",
        "    modified_assistant_response = json.dumps(response_data)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    training_dataset.append(modified_assistant_response)\n",
        "    print(\"Generating traing data... \" + str(i + 1) + \"/\" + str(number_of_datasets_to_generate))\n",
        "\n",
        "  return training_dataset;\n",
        "\n",
        "training_dataset = generate_training_dataset(client, number_of_datasets_to_generate)"
      ],
      "metadata": {
        "id": "6036fpoEP22M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cf2fad-9356-43eb-cf0d-3d7ed8628642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating traing data... 1/3\n",
            "Generating traing data... 2/3\n",
            "Generating traing data... 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vkKUxfe485s",
        "outputId": "79ec73a2-e6bc-47cf-e8db-394bc62f5c0c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully generated training examples.\n",
            "JSON file has been created successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"Successfully generated training examples.\")\n",
        "\n",
        "with open(\"training_dataset.json\", \"w\") as f:\n",
        "  for message in training_dataset:\n",
        "    f.write(message)\n",
        "\n",
        "print(\"JSON file has been created successfully!\")"
      ]
    }
  ]
}